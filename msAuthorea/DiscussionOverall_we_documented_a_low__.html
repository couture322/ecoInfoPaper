<h1 class="ltx_title_section">Discussion<br></h1><div>







<br></div><div>Overall we
documented a low rate of data recovery, with data successfully recovered from
28% of all funded projects. Of the tested project characteristics only the
research field of the project influenced whether the data would be recovered
(Figure 2). In contrast to our expectations, we did not observe a greater
recovery rate among more recent projects, although we did observe a
statistically insignificant increase in the fraction of projects for which data
recovery was successful in the most recent year studied (SuppFigure 2).
Recovery rate did not vary with agency type either, although we similarly had
reasons to assume differences would exist (SuppFigure 1).<br></div><div>&nbsp;<br></div><div>The low recovery rate (28%) we obtained is similar to published
data recovery efforts testing journal specific projects (see Wollins 1962 who
reported a 24% recovery rate, Wichert et al. 2006 who reported 26%, Savage
&amp; Vickers 2009 who reported 10%, Vines et al. 2014 who report 19% recovery).
Compared to these studies we tested publicly funded projects whose data were created
under an agreement of public ownership. We also invested considerably more
effort in data recovery in terms of person hours, number of contact attempts,
and one-on-one data support relative to these previous studies. Furthermore, although
many projects comprised multiple datasets, we considered success to be the receipt
of any (but not necessarily all) data associated with a funded project.
Therefore, our estimate of 28% recovery is conservative and likely masks a much
lower recovery rate at the dataset level. <br></div><div>&nbsp;<br></div><div>The similarity in results suggests that although funders and
publishers have unique incentives for data sharing researchers will share data
at the same rate. In a project such as this, the funders actually own the data,
whereas journals are the gatekeepers of manuscript publication requiring
publication or sharing of data over which they can claim no official ownership.
Differentiating between a moral versus ambition-motivated data sharing impetus appears
to be moot as each result in similarly low data sharing rates. This result is
unsurprising given that both journals and funders have long held these data
sharing requirements but rarely follow up on compliance and therefore neglect
to incorporate adherence into decisions of future benefits (future publications
or future funding awards, respectively), an issue we address in depth below. What
does differ more than <i>who</i> requires
data sharing, is the field of study in which the data are collected.<br></div><div>&nbsp;<br></div><div>Of the three project characteristics we tested, only research
field had an impact on whether data were recovered. Within this grouping,
research fields of oil, physical oceanography, plankton, and benthic
invertebrates were more likely to share data than the fields of fisheries,
birds, mammals, habitat, modeling, and social studies. Differences in recovery
rates by research field may be attributable, in part, to the time required to
prepare data for sharing as well as whether or not certain data types are
considered difficult to interpret leading to concerns about appropriate use by
other scientists.<br></div><div>&nbsp;<br></div><div>Those research
fields that were more likely to share data tend to have more streamlined
procedures and data collection processes. Almost all of the oil data sets
sought during this data salvaging project were collected by a single agency, so
it was easy to obtain many data sets once communication with that agency was
established (of course the alternative would have been true if communication
was stalled). Furthermore, these data were likely managed more carefully from
the beginning due to the legal relevance of oil records following the oil
spill.&nbsp; Meanwhile, both physical
oceanography and plankton data sets are often collected using systematic and
automated approaches. Once data are collected, they often do not require manual
data entry or post-processing and are produced in a final format that is ready
to be shared. Previous research indicates data sharing increases with the level
of data collection automation&nbsp;(Pritchard et al. 2004). We postulate that
researchers may also be likely to share benthic invertebrate data because these
are collected using direct observation, using well-established methods. Again,
here the data are ready to share relatively soon without post-processing or
fear of misinterpretation.<br></div><div>&nbsp;<br></div><div><i>This fear
of misuse or misunderstanding is more common in the fields that were more
likely to not share data. </i><i>Fisheries data are
often sensitive in nature their frequent use in assessing vulnerable stocks
that are fished by diverse groups, setting management controls such as total
allowable catch limits, or monitoring the catch of sensitive or protected
species. This can understandably raise concern with stakeholders who may worry
that sharing their data could reveal prized fishing grounds, result in more
restrictive or inequitable management, reveal illegal fishing patterns, or even
lead to a fishery or particular areas being closed (Froese et al 2004;
Mackinson et al 2011).</i><i> </i><i>Meanwhile, the
sharing of bird and mammal data faces its own unique challenges due to
non-standardization of methods and data complexities. For these populations
with global ranges, determining population size from discrete observations
takes deeper knowledge of the organism and specific population, with no
recognized standard for how these data should be structured researchers may be
reluctant to share data with scientists unfamiliar with internal protocols
(Huettmann 2011). At the same time, researchers studying marine mammals also
work with small populations, such as dolphins and sea otters, with more
localized ranges. Researchers may be protective over their research population
due to a developed intimacy with their subjects and fear that disclosing their
locations could increase their poaching risk (Meyer 2009). Finally, recovery of
social science data shares many of the same obstacles but many of the datasets
involve human subjects so might fear risking the privacy and confidentiality of
their subjects if data are mishandled. Methods do exist to protect against such
exposure, but these steps require additional post-processing, inhibiting
effortless sharing (Fienberg et al. 1985, Sieber 1991, King 2011). </i><br></div><div>&nbsp;<br></div><div>One of our main
hypotheses was that data from older projects would be less accessible (Michener
1997), but found limited evidence to support this, and did not observe a
gradual decrease in data recovery over time as expected. We did, however,
observe the greatest success rate in the most recent year, in general agreement
with Michener's finding that data recovery rates decrease with time since data
collection (Michener 1997). The expected gradual decline may have been obscured
by two artifacts of our methods: 1) Here data recovery rates were based on
projects instead of datasets, which may have masked success over time. 2) Since
we calculated the “age” of data based on only the last year of the project, few
projects happened to end in the most recent years sought, especially since
EVOSTC funded projects tended towards longer monitoring and synthesis work in
later years (SuppFigure 2).&nbsp;Furthermore, the lack of temporal trend in
information-dependent reasons for not sharing (e.g., Data loss, availability of
contact information, non-digital data) led us to conclude that the Michener
temporal trend in data information was not supported by this data recovery
effort.<br></div><div>&nbsp;<br></div><div>Inherent in all of
the above reasons we report for non-recovery of data is the absence of a reward
structure for sharing data. There is no reward for the time investment required
to learn how to curate data or reconstruct data packages (including error-free
data, project and file level metadata) where original data curation was
insufficient. The application of digital object identifiers (DOIs) to datasets
allows data to be cited in the same way as publications. Although this is a relatively
new practice, as DOIs become more widely used, data citations should be
incorporated into a scientists’ overall research output along with journal
publications. Furthermore, once funding is delivered and papers are published,
funding agencies and journals have little power to enforce data sharing
requirements. These groups should enforce these requirements more stringently
and provide data tools and assistance to its users. For example, the National
Science Foundation has created a dedicated data repository, the <a href="file:///C:\Users\rblake\AppData\Local\Temp\arcticdata.io">NSF Arctic Data Center</a> (ACD), where all
Arctic data projects funded by NSF can be permanently archived, ensuring proper
fulfillment of the data management requirements for their grants. In addition
to the infrastructure, the NSF ADC also employs a data support team to assist in
data documentation and attribution helping to minimize the work this
requirement may add. NSF enforces these requirements by checking for data
archiving in the ADC, or acceptable substitutes, before any additional NSF
funding will be granted.<br></div><div><div><div><br></div><br></div><br></div><div><br></div>