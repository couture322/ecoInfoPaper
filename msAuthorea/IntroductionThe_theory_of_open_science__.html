<h1 class="ltx_title_section">Introduction<br></h1><div>The theory of open science is gaining popularity worldwide
but many scientists are finding actual application to be much more difficult ().
Open science refers to the public sharing of the entire scientific process from
initial brainstorm to publication and a number of tools have been developed to
facilitate openness in each of these areas, but even those deeply involved in its
development acknowledge that the movement is still in its adolescence (). <br></div><div>&nbsp;<br></div><div>As the building block of science, data is at the crux of
opening up science, but the idea of sharing data is met with both conceptual
and practical obstacles. The culture of ownership over one’s data combined with
competition for funding and publications slows the adoption of data sharing
practices. Scientist fear ideas will be stolen or publication opportunities lost
if the data are published prematurely or at all. Additionally, many of our
commonly used tools neglect to prepare data for publication or sharing and
therefore require additional training, effort and resources to complete this
extra step. While tools have been developed to help scientists responsibly
archive their data in a manner that will be accessible and preserved through
time these tools and an understanding of their appropriate use are still sparsely
available to scientists. William Michener hypothesizes that there is a temporal
trend in even the <i>knowledge</i> of one’s
own data, and concludes that the older data are, the less information exists
for both the data itself and associated metadata (Michener 1997). Given this
conclusion it is realistic to assume that older the data should the less
available than newer data. Furthermore, changes in technologies should further
support this increase in barriers over time, as archiving and file formats are
obsoleted. <br></div><div>&nbsp;<br></div><div>A number of studies have tested the availability of data
under open publication requirements, some dating back to the 1960s. These
studies have focused on relatively small sample sizes and data acquisition
efforts but have consistently recovered only 20-30% of the data requested
(Wolins 1962, Wicherts et al. 2006, Savage &amp; Vickers 2009). We wanted to
test whether a larger and more modern effort would return better results than
these older and smaller studies and if not, determine why data were not
acquired. <br></div><div>&nbsp;<br></div><div>In 2010 the Exxon Valdez Oil Spill Trustee Council (EVOSTC)
funded the Gulf Watch Alaska group to create an open archive of the data
collected under their grants. The EVOSTC was created following the Exxon Valdez
oil spill in the Gulf of Alaska in 1989, and has funded hundreds of projects
since then, therefore the data sought would span over two decades. The EVOSTC
required publication of data within one year of data collection for all of
their grants but did not specify or provide a publication platform. Grants
funded a variety of disciplines with mainly a marine focus and grantees
included government agencies, private consulting firms and non-governmental
organizations, and Alaska native groups. We wanted to know for how many of
these projects we could acquire data, if there were trends in data reporting
based on data or grantee characteristics and if data were not procured, why we
were unsuccessful.<br><br></div><div><br></div><div><br></div>