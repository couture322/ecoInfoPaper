<h1 class="ltx_title_section">Discussion<br></h1><div>Colette is working on this section (in progress).<br></div><div>Gavin finished sub-section on why certain research topics are more likely to share than others (i.e., oil, plankton, fisheries, social, etc).<br></div><div><br></div><div><b>NB: Discussion outline is in bold.  <i>Notes / comments to you guys are </i></b><i><b>italicized</b></i><b><i>.</i></b><br></div><div><br></div><div><b>Discussion outline:</b><br></div><div><b>first re-state results without going into major detail again:</b><br></div><div><b>we recovered 26% of data</b><br></div><div><b>temporal: nope.</b><br></div><div><b>sectors: no diferences between agency type</b><br></div><div><b>there were differences between research fields (more data recovered from physical oceanography)</b><br></div><div>Overall we documented a low rate of data recovery, with data successfully recovered from 26% of all funded projects. Because we considered success to be the receipt of any (but potentially not all) data associated with a funded project (which often comprised multiple datasets), our estimate is conservative and likely masks a much lower recovery rate at the dataset level. In contrast to our expectations, we did not observe a greater recovery rate among more recent projects, although we did observe a statistically insignificant increase in the fraction of projects for which data recovery was successful in the two most recent years of study (<i>is this correct?</i>). Recovery rate did not vary with agency type, but did vary with research field.<br></div><div><br></div><div><br></div><li><div><i><b>although there have been Reporting requirements in place for a while, recovery rate hasn't changed and people still don't share ...</b></i><br></div></li><ul><li><div><i><b>Publishers</b></i><br></div></li><li><div><i><b>Funders</b></i><br></div></li><li><div><b><i>...But still not happening<br><br></i></b><br></div></li></ul><div><i>I think we can roll the next 3 points (I'm numbering them </i><b><i>1-3 </i></b><i>below for clarity when we talk about this, not for the ms) into a single 'first' sentence, then branch out from there (these could probably all be a single paragraph, or more if anyone wants to expand):</i><br></div><div><div><br></div><div><b>*</b>We document a data recovery rate
similar to that reported by Wollins (1962) over 50 years ago, despite the advent
of <u>several factors <i>(ie 1-3 below)</i></u> in the intervening time period which should have promoted and facilitated data sharing.&nbsp; Our recovery rate is also similar to more recent reports  (Wichert et al. 2006, Savage &amp; Vickers 2009).<b>(1) </b>First, recent implementations of data sharing requirements by funding agencies and publishers did not appear to increase data recovery rates relative to that of Wollins (xx %; 1962), 
which was reported prior to the introduction of  such requirements. We 
also noted a 
similar success rate to that reported in 2006 (Wichert et al. 2006) and 2009 (Savage &amp; Vickers 2009), both of which requested data associated with publications whose journals required data sharing on 
request. Notably, we  invested considerably more effort in data recovery (person
hours, number of projects for which we requested data, number of contact attempts, and an
electronic tracking system) relative to these 3 studies, without a noticeable
effect on recovery success.<br></div><br></div><ul><li><div><i><b>Open science movement:</b></i><br></div></li><ul><li><div><i><b>Data sharing is important … but there are challenges</b></i><br></div></li></ul><div><div><i>Not sure what we wanted to put here? </i><br></div><div><b>(2) </b>The advent of a culture of data sharing also had no apparent effect on recovery success relative to historical success rates.<br></div><div><br></div><br></div></ul><li><div><i><b>Historical assessments of data sharing </b></i><br></div></li><ul><li><div><div><div><b><i>altough we didn't do a metaanalysis, recovery rates are similar to 50 yrs ago (note I've folded this into * above)<br></i></b><br></div><div><div><i>Seems a good spot to bring in that tech advances did not change recovery rates ... it fits into the historical comparison. So I'm adding that here:<br></i><b>(3) </b>The introduction of technologies facilitating data storage and transfer  also appeared to have no impact on data recovery rates relative to historical success rates. Technological advances in data capture, storage (including large volume storage), accessibility,
and ease of duplication and transfer (including electronic data formats and email
availability) should have facilitated data recovery, but did not. This is supported by our observation that (i) only a small proportion of data we received was non-digital  (4%, n = 10), 
and (ii) non-recovery attributable to the 'other' category (which included obsolete technology and discarded files) was also low (7% of projects, n = 17).&nbsp;Notably,
X% of this data loss was attributable to loss of data by the funding body (ie <i>the
infamous CD)</i>.&nbsp; Furthermore, we saw no
difference in recovery rate between projects conducted before and after 2000
<i>(**or some other way of marking the digital age?**</i>) – there was a similar
success rate for all projects conducted &gt;5 years prior to our data request.<br></div><br></div><br></div><br></div></li></ul><div>Instead, non-recovery of data was most commonly attributable to loss of contact with data holders (50%, n = 116)<i> </i>and lack
of contact information (35%, n = 82), suggesting that a lack of 
foresight for long-term data archiving (on the part of data PIs and the funding agency) was the most important barrier to 
data sharing ... <i><b>Jessica, were these reasons uniformally 
distributed across agency type and reserarch sector? If so, then we can 
say this was a universal pattern.</b></i> This lack of foresight 
encompasses the lack of planning to handle attrition of personnel 
(movement of personnel between institutions, retirement, death) and may 
also reflect an absence of adequate data curation, rendering it 
difficult and time consuming to reconstruct data
 packages suitable for use by others unfamiliar with the original work 
(Wichert et al. 2006, Savage &amp; Vickers 2009). The latter is 
supported by the fact that several data PIs 
(1%, n = 3)<i> </i>requested funding for the time they would need  to 
format data for sharing, despite having agreed to share data upon 
acceptance of the original funding. The issue is exacerbated by a frequent lack of institutional support within government agencies(e.g. lack of dedicated funds and, by extension, personnel)&nbsp;to process data sharing requests.&nbsp;  <i> <i>I feel we should take care 
not to single out our data PIs for poor data curation. This is almost 
certainly a widespread issue in ecology and it wasn't until very recently 
that people started to think about data curation for the long-term. Not 
sure how to say this.</i></i><br></div><div><br></div><div><br></div><div><br></div><div>One of our main research questions was why didn't people share their data. There were no significant differences between research sectors, but is this likely because we lacked the statistical power to detect significant differences <i>(E.g. were the stats tests not significant because the equation for the test statistic is unable to find a significant difference between a category with many members (eg 120 in government) vs a category with very few members, eg 3 in tribes)? - Jessica - can you expand on this just a bit to fill in the stats?) &nbsp;</i><br></div><div><br></div><div>We noted statistically significant variation in data recovery between research topics, suggesting that the research topics of oil, physical oceanography,  plankton, and invertebrates were more likely to share their data than the topics of fisheries, birds, mammals, habitat, modeling, and social studies. We expand on potential reasons why below. In general, differences in recovery rates by study category may be attributable, in part, to the time required to prepare data for sharing as well as whether or not certain data types could be misused or misinterpreted.<br></div><div><br></div><div>There are a number of reasons specific to the topics of&nbsp;oil, physical oceanography, plankton, and invertebrates which may explain why researchers in these fields are likely to share their data. Many of the oil data sets for this data salvaging project were collected by a single agency (<i>Jessica - what was this agency?</i>), so it was straightforward to obtain many data sets once communication with that agency was opened. Meanwhile, both physical oceanography and plankton data sets (<i>Gavan is it ok to remove plankton from here? plankton often require ID and enumeration under miscroscope by humanoids)</i> are often collected using systematic and automated approaches. Once data are collected, they often do not require manual data entry but are already in a final format that is ready to be shared. Previous research indicates data sharing increases with the level of data collection  automation&nbsp;[Pritchard et al. 2004]. We postulate that researchers may also be likely to share <i>(Gavan: can I add 'benthic' here?</i>) invetebrate data because these are collected using direct observation (i.e., percent cover), and again are ready to share immediately without post-processing or fear of misinterpretation.<br></div><div><div><br></div><div>There are several reasons why researchers in fields relating to fisheries, birds, mammals, and social studies may not share their data.&nbsp;While data sharing and data recovery between scientists, stakeholders, and institutions is a challenge for many fields, the field of fisheries poses unique challenges that may also require unique solutions to overcome. Fisheries data are often sensitive in nature. They may be used for assessing vulnerable stocks that are fished by diverse groups, setting management controls such as total allowable catch limits, or monitoring the catch of sensitive or protected species. This can understandably raise concern with stakeholders who may worry that sharing their data could reveal prized fishing grounds, result in more restrictive or inequitable management, reveal illegal fishing patterns, or even lead to a fishery or particular areas being closed [Froese et al 2004; Mackinson et al 2011]. To overcome these concerns, a shared culture and strategy of collaborative research and data management should be developed between scientists, fishers, decision-makers, and other stakeholders from the onset of any new fisheries data collection program. Through this approach, shared expectations and objectives can be set such that all stakeholders understand how data will be used and how this may impact management decisions [CEC 2009; Johnson et al. 2007; Wiber et al. 2009].<br></div><br></div><div>Meanwhile, bird and mammal data sharing faces its own unique challenges. Bird data is often collected using citizen science, where the observers have varying levels of training and identification expertise [Borgman 2012]. Researchers may therefore be hesitant to share these data with a broader audience over fears of misinterpretation or misuse. <i>(Jessica: were our bird data from citizen science?)</i> Bird data also often includes estimates of population abundance. However, estimating population abundance could be obtained from direct or indirect observation, each of which come with certain assumptions and sampling techniques, making their integration into larger analyses non-trivial; researchers may again be concerned their studies could be misused [Baker and Millerand 2010]. Specifically for seabirds, their associated data is often complex with focuses on sitings, nesting success, and colony size. However, there is currently no recognized standard for how these data should be structured [Huettmann 2011]. Meanwhile, researchers studying marine mammals often work with small populations with limited geographic coverage, such as dolphins that may not travel widely. These researchers may be more protective over their research population since they have spent so much time and effort trying to understand it [Meyer 2009]. Additionally, for marine mammal photo ID projects, there is no common naming, format, or filing standard [Baker and Millerand 2010].<br></div><div><br></div><div><div>Finally, recovery of social science data also shares many of the same challenges as those faced with other data types, but comes with its own unique reasons why data sharing may be more limited. In particular, there may be concerns with certain types of social science data sets in which individual human subjects could be identified, which poses the risk of privacy and confidentiality violations if data are mishandled. These concerns could be mitigated, however, by protection of human subject anonymity through the deletion of identifiers,&nbsp;aggregation of data away from individuals, and privacy-enhanced data sharing protocols and intellectual property rules [Fienberg et al. 1985, Sieber 1991, King 2011].<br></div><br></div><div><b>References for discussion (this covers the above 5 paragraphs on data sharing by research topic)</b><br></div><ul><li><a href="http://www.vliz.be/en/imis?refid=209721">http://www.vliz.be/en/imis?refid=209721</a><br></li><li><a href="http://www.sciencedirect.com/science/article/pii/S0308597X10001375">http://www.sciencedirect.com/science/article/pii/S0308597X10001375</a><br></li><li><a href="http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=COM:2009:0163:FIN:EN:PDF">http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=COM:2009:0163:FIN:EN:PDF</a><br></li><li><a href="http://icesjms.oxfordjournals.org/content/64/4/834.short">http://icesjms.oxfordjournals.org/content/64/4/834.short</a><br></li><li><a href="http://www.sciencedirect.com/science/article/pii/S0308597X08000973">http://www.sciencedirect.com/science/article/pii/S0308597X08000973</a><br></li><li><a href="http://www.sciencedirect.com/science/article/pii/S0308597X08000973">https://books.google.com/books?hl=en&amp;lr=&amp;id=ijUrAAAAYAAJ&amp;oi=fnd&amp;pg=PR1&amp;dq=sharing+research+data+fienberg&amp;ots=bG0vHnkNzp&amp;sig=MVHLCiJJ-1BQeZ8G_TaGcvNAY70#v=onepage&amp;q=sharing%20research%20data%20fienberg&amp;f=false</a><br></li><li><a href="http://www.sciencedirect.com/science/article/pii/S0308597X08000973">https://books.google.com/books?hl=en&amp;lr=&amp;id=-O1yAwAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;dq=%22data+sharing%22+%22social+science%22&amp;ots=qSuv7t8XxK&amp;sig=WvQQt48cEB1R4xSbmGlecos2aG4#v=onepage&amp;q=%22data%20sharing%22%20%22social%20science%22&amp;f=false</a><br></li><li><a href="http://www.sciencedirect.com/science/article/pii/S0308597X08000973">http://science.sciencemag.org/content/331/6018/719.full</a><br></li><li><a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.22634/full">http://onlinelibrary.wiley.com/doi/10.1002/asi.22634/full</a><br></li><li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.466.473&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.466.473&amp;rep=rep1&amp;type=pdf</a><br></li><li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.466.473&amp;rep=rep1&amp;type=pdf">http://bentham-open.com/contents/pdf/TOOENIJ/TOOENIJ-4-1.pdf</a><br></li><li><a href="http://www.immagic.com/eLibrary/ARCHIVES/GENERAL/UCSB_US/S040823P.pdf">http://www.immagic.com/eLibrary/ARCHIVES/GENERAL/UCSB_US/S040823P.pdf</a><br></li></ul><div><div><br></div><div><br></div><div><br></div><div><br></div><div>Inherent in all of the above reasons we report for non-recovery of data is the absence of a reward structure for sharing data. There is no reward for the time investment required to (i) learn how to curate data, nor (ii) reconstruct data packages where data curation was insufficient. Furthermore, once funding is delivered and once papers are published, funding agencies and journals have little power to enforce data sharing requirements. <i>some people only shared when they were seeking continuation of
funding (GWA; find a diplomatic way to say this)</i><br></div><div><br></div><br></div><div><br></div><ul><li><i><b>segue to discussion of temporal result</b></i><br></li></ul><div><i>What
 can we say about this, besides that we did not observe the expected 
temporal trend predicted by Michener? Can you guys think of reasons why 
we did't see it? All I can think of is that our use of project-level 
instead of dataset-level may have masked the expected trend? <i>Were 
there temporal trends in reasons for non-recovery? e.g. more 
non-recovery in earlier years due to data loss and non-digital formats?</i></i><br></div><div>Although we did not observe a gradual decrease
 in data recovery over time, we did observe a greater success rate in 
the most recent years, which generally agrees with Michener's (1997) finding 
that data recovery rates decrease with time since data collection; that
 we didn't see the gradual decline may be an artifact of evaluating 
projects instead of datasets.<br></div><div><br></div><div><br></div><div><br></div><div><div><br></div><br></div><div>-----------------------------------------------------------------<br></div><div><br></div><div>Notes
 from October 7 call - we only include data recovery efforts from 
defined 2 year period, even though some data and digitization has 
trickled in since then. Not every effort will have the luxury of 
continued staff time after a defined period. And this shouldn't affect 
analysis too much anyway, as it's a very small chunk of the recovered 
data.<br></div><div><br></div><div>near to concluding statement:<br></div><div>though there always been reporting requirements, new efforts at open 
science and with modern technology and tools, perhaps we can move 
science foreward (hope for sharing) with open data sharing. yay for data repositories.<br></div><div><div><br></div><div><br></div><div>-----------------------------------------------------------------<br></div><div><br></div><div><i>Extra material left over from first draft of discussion:</i><br></div><div>In broader terms, many of the  commonly reported reasons for not sharing data point to a high perceived risk for sharing data: <br></div><div>(i) concern about others finding errors in their results &amp; conclusions (see
speculation by all 3 other published studies), <br></div><div>(ii) retaining data ownership / authorship rights. We could cite the 2015-2016 series
of&nbsp; editorials / letters / comments in
Trends Ecol. Evol.:<br></div><div>a) Archiving
Primary Data: Solutions for Long-Term Studies <a href="http://www.cell.com/trends/ecology-evolution/fulltext/S0169-5347%2815%2900185-8"><b>http://www.cell.com/trends/ecology-evolution/fulltext/S0169-5347(15)00185-8</b></a><br></div><div>b) A Balanced Data
Archiving Policy for Long-Term Studies<br></div><div><a href="http://www.cell.com/trends/ecology-evolution/abstract/S0169-5347%2815%2900298-0"><b>http://www.cell.com/trends/ecology-evolution/abstract/S0169-5347(15)00298-0</b></a><br></div><div>c) Solutions for
Archiving Data in Long-Term Studies: A Reply to Whitlock <i><b>et
al.</b></i> <a href="http://www.cell.com/trends/ecology-evolution/abstract/S0169-5347%2815%2900301-8"><b>http://www.cell.com/trends/ecology-evolution/abstract/S0169-5347(15)00301-8</b></a><br></div><div>d) Also some food
for thought here, arising from a Jan 2016 editorial in the New England Journal
of Medicine ... especially use of the term 'research parasites': <a href="http://simplystatistics.org/2016/01/25/on-research-parasites-and-internet-mobs-lets-try-to-solve-the-real-problem/">http://simplystatistics.org/2016/01/25/on-research-parasites-and-internet-mobs-lets-try-to-solve-the-real-problem/</a><br></div><div><br></div><div><br></div><br></div><div><br></div><div><br></div>